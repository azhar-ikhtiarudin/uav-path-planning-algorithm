{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDL8PCVketc3"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFU-N35_etc-"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import tensorboard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import PIL\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeGQm_-getdA",
        "outputId": "c9fc187a-7af0-4478-9574-a4da59449677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "# GPU Check\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYqGuLpZetdC"
      },
      "source": [
        "## Parameters Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHhTsSBJetdC"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "MAP_X = 30\n",
        "MAP_Y = 20\n",
        "GRID_SIZE = 0.5\n",
        "SIZE_X = int(MAP_X/GRID_SIZE)\n",
        "SIZE_Y = int(MAP_Y/GRID_SIZE)\n",
        "\n",
        "pathDist_path = f'{MAP_X}x{MAP_Y}_pathDist.csv'\n",
        "\n",
        "# Init variables and constants\n",
        "#region\n",
        "STATE_SIZE = 1\n",
        "ACTION_SIZE = 8\n",
        "MODEL_NAME = 'Model_Name'\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001\n",
        "DISCOUNT = 0.99\n",
        "MAX_STEP = 1000\n",
        "\n",
        "# Environment settings\n",
        "####################################################\n",
        "EPISODES = 2000 \n",
        "####################################################\n",
        "\n",
        "\n",
        "####################################################\n",
        "REPLAY_MEMORY_SIZE = 5000 # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = 100\n",
        "MINIBATCH_SIZE = 10  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "####################################################\n",
        "\n",
        "MEMORY_FRACTION = 0.20\n",
        "\n",
        "#  Stats settings\n",
        "AGGREGATE_STATS_EVERY = 50  # episodes\n",
        "SHOW_PREVIEW = False\n",
        "#endregion\n",
        "\n",
        "# Init variables and constants\n",
        "\n",
        "MIN_REWARD = -200  # For model save\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smJtjRLSUaFr"
      },
      "source": [
        "## ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OnqREULUXD4"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "\n",
        "def createLine(walls, SIZE_Y, SIZE_X):\n",
        "    # Full Horizontal Line\n",
        "    # for i in range(SIZE_X):\n",
        "    #     walls[int(SIZE_Y/2),int(i)] = 1\n",
        "\n",
        "    # Full Vertical Line\n",
        "    # for i in range(SIZE_Y):\n",
        "    # walls[int(i), int(SIZE_X/2)] = 1\n",
        "\n",
        "    # Segment 1\n",
        "    for i in range(SIZE_Y//10, SIZE_Y//3):\n",
        "        walls[i, SIZE_X//2] = 1\n",
        "\n",
        "    # Segment 2\n",
        "    for i in range(SIZE_X//2, SIZE_X//2+SIZE_X//6):\n",
        "        walls[SIZE_Y//10, i] = 1\n",
        "\n",
        "    # Segment 3\n",
        "    for i in range(SIZE_Y//10, SIZE_Y//4):\n",
        "        walls[i, SIZE_X//2+SIZE_X//6] = 1\n",
        "\n",
        "    # Segment 4\n",
        "    for i in range(SIZE_X//2-SIZE_X//6, SIZE_X//2+1):\n",
        "        walls[SIZE_Y//3, i] = 1\n",
        "\n",
        "    # Segment 5\n",
        "    for i in range(SIZE_Y//3, SIZE_Y//3+SIZE_Y//4):\n",
        "        walls[i, SIZE_X//2-SIZE_X//6] = 1\n",
        "\n",
        "    # Segment 6\n",
        "    for i in range(SIZE_X//2-SIZE_X//6, SIZE_X//2+SIZE_X//6):\n",
        "        walls[SIZE_Y//3+SIZE_Y//4, i] = 1\n",
        "\n",
        "    # Segment 7\n",
        "    for i in range(SIZE_Y//3+SIZE_Y//4, SIZE_Y//3+SIZE_Y//4+SIZE_Y//5):\n",
        "        walls[i, SIZE_X//2+SIZE_X//6] = 1\n",
        "\n",
        "    # Segment 8\n",
        "    for i in range(SIZE_X//2+SIZE_X//6, SIZE_X//2+SIZE_X//6+SIZE_X//6):\n",
        "        walls[SIZE_Y//3+SIZE_Y//4+SIZE_Y//5, i] = 1\n",
        "\n",
        "    # Segment 9\n",
        "    for i in range(SIZE_Y-SIZE_Y//3, SIZE_Y):\n",
        "        walls[i, SIZE_X//2] = 1\n",
        "\n",
        "    # Segment 10\n",
        "    for i in range(SIZE_Y-SIZE_Y//4, SIZE_Y):\n",
        "        walls[i, SIZE_X//2-SIZE_X//4] = 1\n",
        "\n",
        "    # Segment 11\n",
        "    for i in range(0, SIZE_X//6):\n",
        "        walls[SIZE_Y//5, i] = 1\n",
        "\n",
        "    # Segment 12\n",
        "    for i in range(SIZE_Y//5, SIZE_Y//5+SIZE_Y//5):\n",
        "        walls[i, SIZE_X//6] = 1\n",
        "\n",
        "    # Segment 13\n",
        "    for i in range(0, SIZE_X//6):\n",
        "        walls[SIZE_Y//5+SIZE_Y//3, i] = 1\n",
        "\n",
        "    # Segment 14\n",
        "    for i in range(0, SIZE_Y//3):\n",
        "        walls[i, SIZE_X-SIZE_X//6] = 1\n",
        "\n",
        "    # Segment 15\n",
        "    for i in range(SIZE_X-SIZE_X//10, SIZE_X):\n",
        "        walls[SIZE_Y//3, i] = 1\n",
        "\n",
        "    # Segment 16\n",
        "    for i in range(SIZE_X-SIZE_X//9, SIZE_X):\n",
        "        walls[SIZE_Y//2+SIZE_Y//10, i] = 1\n",
        "\n",
        "    # Segment 17\n",
        "    for i in range(SIZE_Y//2+SIZE_Y//10, SIZE_Y//2+SIZE_Y//10+SIZE_Y//4):\n",
        "        walls[i, SIZE_X-SIZE_X//9] = 1\n",
        "\n",
        "    # Segment 18\n",
        "    for i in range(SIZE_Y//2+SIZE_Y//10+SIZE_Y//3, SIZE_Y):\n",
        "        walls[i, SIZE_X-SIZE_X//9] = 1\n",
        "\n",
        "    return walls\n",
        "\n",
        "\n",
        "class EnvObject:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.x}, {self.y}\"\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return (self.x - other.x, self.y - other.y)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.x == other.x and self.y == other.y\n",
        "\n",
        "\n",
        "class Drone(EnvObject):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__(x, y)\n",
        "\n",
        "    def action(self, choice, walls = None):\n",
        "        '''\n",
        "        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
        "        '''\n",
        "        #if choice == 0:\n",
        "            #x = 0\n",
        "            #y = 0\n",
        "            \n",
        "        if choice == 0:\n",
        "            x=-1\n",
        "            y=-1\n",
        "        elif choice == 1:\n",
        "            x=-1\n",
        "            y=1\n",
        "        elif choice == 2:\n",
        "            x=1\n",
        "            y=-1\n",
        "        elif choice == 3:\n",
        "            x=1\n",
        "            y=1\n",
        "            \n",
        "        elif choice == 4:\n",
        "            x=1\n",
        "            y=0\n",
        "        elif choice == 5:\n",
        "            x=-1\n",
        "            y=0\n",
        "            \n",
        "        elif choice == 6:\n",
        "            x=0\n",
        "            y=1\n",
        "        elif choice == 7:\n",
        "            x=0\n",
        "            y=-1\n",
        "        \n",
        "        self.move(x=x, y=y, walls=walls)\n",
        "            \n",
        "\n",
        "    def move(self, x=0, y=0, walls = None):\n",
        "        #if no value for x or y, stay\n",
        "        \n",
        "        x,y = self.collisionCheck(x,y,walls)\n",
        "        self.x += x\n",
        "        self.y += y\n",
        "\n",
        "            \n",
        "    def collisionCheck(self, x=0, y=0, walls=None):\n",
        "        #checking for out of bounds\n",
        "        predict_x = self.x + x\n",
        "        predict_y = self.y + y\n",
        "        if predict_x < 0 or predict_x > SIZE_X-1:\n",
        "            x = 0\n",
        "        if predict_y < 0 or predict_y > SIZE_Y-1:\n",
        "            y = 0\n",
        "            \n",
        "        \n",
        "        # Check for collision with walls    \n",
        "        if walls is None:\n",
        "            return x,y\n",
        "        elif walls[self.y+y][self.x+x] == 1:\n",
        "            return 0,0\n",
        "        else:\n",
        "            return x,y\n",
        "        \n",
        "        # # Several edge cases exist so we'll implement a simpler system\n",
        "        # for i in range(np.sign(x), x + np.sign(x), np.sign(x)):\n",
        "        #     if walls[self.y][self.x+i] == 1:\n",
        "        #         x = i - np.sign(x)\n",
        "        #         break\n",
        "            \n",
        "        # for j in range(np.sign(y), y + np.sign(y), np.sign(y)):\n",
        "        #     if walls[self.y+j][self.x] == 1:\n",
        "        #         y = i - np.sign(y)\n",
        "        #         break\n",
        "        \n",
        "\n",
        "class Target(EnvObject):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__(x, y)\n",
        "\n",
        "\n",
        "class DroneEnv:\n",
        "    # Define Parameters\n",
        "    SIZE_X = SIZE_X\n",
        "    SIZE_Y = SIZE_Y\n",
        "    ENV_COLOR = (20, 52, 89)\n",
        "    WALLS_COLOR = (77, 77, 234)\n",
        "    DRONE_COLOR = (234, 222, 53)\n",
        "    TARGET_COLOR = (132, 234, 53)\n",
        "    space = np.zeros((SIZE_Y, SIZE_X, 3), dtype=np.uint8)\n",
        "    walls = createLine(\n",
        "        np.zeros((SIZE_Y, SIZE_X), dtype=np.uint8), SIZE_Y, SIZE_X)\n",
        "    pathDist = pd.read_csv(pathDist_path, header=None, dtype='Int32').values\n",
        "    \n",
        "    def reset(self):\n",
        "        self.agent_1 = Drone(self.SIZE_X-self.SIZE_X//11, self.SIZE_Y//10)\n",
        "        self.agent_2 = Drone(self.SIZE_X-self.SIZE_X//20,\n",
        "                             self.SIZE_Y//2-self.SIZE_Y//20)\n",
        "        self.agent_3 = Drone(self.SIZE_X-self.SIZE_X//13,\n",
        "                             self.SIZE_Y-self.SIZE_Y//11)\n",
        "        self.target = Target(self.SIZE_X//10, self.SIZE_Y//2-self.SIZE_Y//20)\n",
        "\n",
        "        self.episode_step = 0\n",
        "\n",
        "        observation = (self.pathDist[self.agent_1.y][self.agent_1.x])\n",
        "        return observation\n",
        "\n",
        "    def step(self, action, observation):\n",
        "        reward = 0\n",
        "        done= False\n",
        "        self.episode_step += 1\n",
        "        self.agent_1.action(action, self.walls)\n",
        "\n",
        "        new_observation = (self.pathDist[self.agent_1.y][self.agent_1.x])\n",
        "\n",
        "        if abs(self.agent_1.x - self.target.x) <= 1 and abs(self.agent_1.y - self.target.y) <= 1:\n",
        "            reward = 100\n",
        "            done = True\n",
        "        elif self.episode_step >= MAX_STEP:\n",
        "            reward = -10\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 1.5*(observation - new_observation) - 0.5\n",
        "            done = False\n",
        "        \n",
        "\n",
        "        return new_observation, reward, done\n",
        "\n",
        "    def is_wall(self, action):\n",
        "        if self.walls[self.agent_1.y+action][self.agent_1.x+action] == 1:\n",
        "            return True\n",
        "\n",
        "    def visualize(self):\n",
        "        for i in range(self.SIZE_Y):\n",
        "            for j in range(self.SIZE_X):\n",
        "                if self.walls[i][j] == 1:\n",
        "                    self.space[i][j] = self.WALLS_COLOR\n",
        "                else:\n",
        "                    self.space[i][j] = self.ENV_COLOR\n",
        "\n",
        "        self.space[self.agent_1.y][self.agent_1.x] = self.DRONE_COLOR\n",
        "        self.space[self.target.y][self.target.x] = self.TARGET_COLOR\n",
        "\n",
        "    def render(self):\n",
        "        self.visualize()\n",
        "        img = Image.fromarray(self.space, 'RGB')\n",
        "        # img = img.resize((1200, 800), resample=Image.Resampling.BOX)\n",
        "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    def saveImage(self, image_path, episode='e', step='s'):\n",
        "        self.visualize()\n",
        "        if not os.path.isdir(image_path):\n",
        "            os.makedirs(image_path)\n",
        "\n",
        "        img = Image.fromarray(self.space, 'RGB')\n",
        "        # img = img.resize((1200, 800), resample=Image.Resampling.BOX)\n",
        "        img_rgb = img.convert('RGB')\n",
        "        img_rgb = img_rgb.save(f'{image_path}/episode_{episode}/image_{episode}_{step}.png')\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "        print('test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GyL4TQkUUwF"
      },
      "source": [
        "## DQNAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhI_kKRaqh6Q"
      },
      "outputs": [],
      "source": [
        "# DQN Agent\n",
        "class ModifiedTensorBoard(TensorBoard):    \n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.step = 1\n",
        "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
        "        self._log_write_dir = self.log_dir\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "        self._train_dir = os.path.join(self._log_write_dir, 'train')\n",
        "        self._train_step = self.model._train_counter\n",
        "\n",
        "        self._val_dir = os.path.join(self._log_write_dir, 'validation')\n",
        "        self._val_step = self.model._test_counter\n",
        "\n",
        "        self._should_write_train_graph = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.update_stats(**logs)\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_train_end(self, _):\n",
        "        pass\n",
        "\n",
        "    def update_stats(self, **stats):\n",
        "        with self.writer.as_default():\n",
        "            for key, value in stats.items():\n",
        "                tf.summary.scalar(key, value, step = self.step)\n",
        "                self.writer.flush()\n",
        "\n",
        "# Main Agent class\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size= state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Main model\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # Target network\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # An array with last n steps for training\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "        # Custom tensorboard object\n",
        "        self.tensorboard = ModifiedTensorBoard(\n",
        "            log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
        "\n",
        "        # Used to count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "        \n",
        "\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(\n",
        "            lr=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Adds step's data to a memory replay array\n",
        "    # (observation space, action, reward, new observation space, done)\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    # Trains main network every step during episode\n",
        "    def train(self, terminal_state, step):\n",
        "\n",
        "        # Start training only if certain number of samples is already saved\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch of random samples from memory replay table\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Get current states from minibatch, then query NN model for Q values\n",
        "        current_states = np.array([transition[0]\n",
        "                                  for transition in minibatch])\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        # Get future states from minibatch, then query NN model for Q values\n",
        "        # When using target network, query it, otherwise main network should be queried\n",
        "        new_current_states = np.array(\n",
        "            [transition[3] for transition in minibatch])\n",
        "        future_qs_list = self.target_model.predict(new_current_states,verbose=0)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # Now we need to enumerate our batches\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "            # almost like with Q Learning, but we use just part of equation here\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # Update Q value for given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        # Fit on all samples as one batch, log only on terminal state\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0,\n",
        "                       shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
        "\n",
        "        # Update target network counter every episode\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "\n",
        "        # If counter reaches set value, update target network with weights of main network\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, STATE_SIZE),verbose = 0)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-2kVeqYUdUZ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIUX-QMvetdO"
      },
      "source": [
        "## Training Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbGRrZDnetdP",
        "outputId": "6dd678da-4095-4e0c-df48-60029ae6c37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "single_m30x20_ExponentialDecay_LessStepPenalty_noAFK_10kMaxSte_withLogs\n"
          ]
        }
      ],
      "source": [
        "# Parameters for training\n",
        "\n",
        "MODEL_NAME = f'single_m{MAP_X}x{MAP_Y}_ExponentialDecay_LessStepPenalty_noAFK_10kMaxSte_withLogs'\n",
        "print(MODEL_NAME)\n",
        "# Environment settings\n",
        "####################################################\n",
        "EPISODES = 1000\n",
        "MAX_STEP = 100000\n",
        "####################################################\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 0.9  # not a constant, going to be decayed\n",
        "\n",
        "EPSILON_DECAY = 0.995\n",
        "MIN_EPSILON = 0.001\n",
        "DISCOUNT = 0.99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiUAjdpetdQ"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBcihsceUcxo",
        "outputId": "836f82fa-6408-40ed-ef53-a8a94cbc3b5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "  0%|          | 1/1000 [14:33<242:25:21, 873.59s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  10508 epsilon:  0.8955 episode_reward:  -5058.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/1000 [21:17<165:31:41, 597.10s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5009 epsilon:  0.8910224999999999 episode_reward:  -2309.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/1000 [33:13<180:23:25, 651.36s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  8958 epsilon:  0.8865673875 episode_reward:  -4283.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 4/1000 [48:13<207:26:36, 749.80s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  11321 epsilon:  0.8821345505625 episode_reward:  -5465.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 5/1000 [56:13<180:18:17, 652.36s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6027 epsilon:  0.8777238778096875 episode_reward:  -2818.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 6/1000 [1:06:10<174:57:54, 633.68s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  7476 epsilon:  0.8733352584206391 episode_reward:  -3542.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 7/1000 [1:44:37<325:38:27, 1180.57s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  28861 epsilon:  0.8689685821285359 episode_reward:  -14235.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 8/1000 [2:01:21<309:51:47, 1124.50s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  12562 epsilon:  0.8646237392178933 episode_reward:  -6085.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 9/1000 [2:05:42<235:14:40, 854.57s/episodes] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  3256 epsilon:  0.8603006205218038 episode_reward:  -1432.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|1         | 10/1000 [2:33:51<305:47:46, 1111.99s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  21052 epsilon:  0.8559991174191948 episode_reward:  -10330.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|1         | 11/1000 [2:50:21<295:15:52, 1074.78s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  12326 epsilon:  0.8517191218320987 episode_reward:  -5967.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|1         | 12/1000 [3:01:04<258:53:27, 943.33s/episodes] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  8005 epsilon:  0.8474605262229382 episode_reward:  -3807.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|1         | 13/1000 [3:15:46<253:35:02, 924.93s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  10958 epsilon:  0.8432232235918236 episode_reward:  -5283.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|1         | 14/1000 [3:21:52<207:05:33, 756.12s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  4526 epsilon:  0.8390071074738644 episode_reward:  -2067.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|1         | 15/1000 [4:01:42<341:36:52, 1248.54s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  29562 epsilon:  0.8348120719364951 episode_reward:  -14585.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|1         | 16/1000 [4:08:16<270:57:47, 991.33s/episodes] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  4880 epsilon:  0.8306380115768126 episode_reward:  -2244.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|1         | 17/1000 [4:40:00<345:37:58, 1265.80s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  23452 epsilon:  0.8264848215189285 episode_reward:  -11530.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|1         | 18/1000 [4:48:12<281:49:42, 1033.18s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6070 epsilon:  0.8223523974113339 episode_reward:  -2839.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|1         | 19/1000 [5:52:56<514:53:05, 1889.49s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  46766 epsilon:  0.8182406354242773 episode_reward:  -23187.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|2         | 20/1000 [6:02:04<404:42:20, 1486.67s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6160 epsilon:  0.8141494322471559 episode_reward:  -2884.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|2         | 21/1000 [6:31:30<427:07:09, 1570.61s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  19791 epsilon:  0.8100786850859201 episode_reward:  -9700.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|2         | 22/1000 [6:47:38<377:32:41, 1389.74s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  10828 epsilon:  0.8060282916604905 episode_reward:  -5218.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|2         | 23/1000 [6:52:21<286:59:35, 1057.50s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  3147 epsilon:  0.801998150202188 episode_reward:  -1378.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|2         | 24/1000 [7:11:43<295:11:06, 1088.80s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  12982 epsilon:  0.7979881594511771 episode_reward:  -6295.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|2         | 25/1000 [7:13:33<215:20:11, 795.09s/episodes] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  1224 epsilon:  0.7939982186539212 episode_reward:  -416.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|2         | 26/1000 [7:17:44<170:57:39, 631.89s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  2797 epsilon:  0.7900282275606516 episode_reward:  -1203.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|2         | 27/1000 [7:26:27<161:58:54, 599.32s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5839 epsilon:  0.7860780864228484 episode_reward:  -2724.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|2         | 28/1000 [8:10:49<328:53:20, 1218.11s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  29603 epsilon:  0.7821476959907341 episode_reward:  -14606.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|2         | 29/1000 [8:34:59<347:17:50, 1287.61s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  16095 epsilon:  0.7782369575107804 episode_reward:  -7852.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|3         | 30/1000 [9:15:47<440:44:49, 1635.76s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  27161 epsilon:  0.7743457727232265 episode_reward:  -13385.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|3         | 31/1000 [10:07:43<559:49:36, 2079.85s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  34425 epsilon:  0.7704740438596104 episode_reward:  -17017.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|3         | 32/1000 [10:15:23<428:34:17, 1593.86s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5065 epsilon:  0.7666216736403123 episode_reward:  -2337.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|3         | 33/1000 [10:39:25<415:53:27, 1548.30s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  15910 epsilon:  0.7627885652721107 episode_reward:  -7759.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|3         | 34/1000 [10:47:16<328:48:06, 1225.35s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5218 epsilon:  0.7589746224457501 episode_reward:  -2413.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|3         | 35/1000 [11:08:51<333:59:31, 1245.98s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  14255 epsilon:  0.7551797493335214 episode_reward:  -6932.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|3         | 36/1000 [11:44:26<405:05:31, 1512.79s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  23577 epsilon:  0.7514038505868538 episode_reward:  -11593.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|3         | 37/1000 [11:50:16<311:22:25, 1164.01s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  3844 epsilon:  0.7476468313339195 episode_reward:  -1726.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|3         | 38/1000 [12:07:27<300:21:24, 1124.00s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  11314 epsilon:  0.7439085971772499 episode_reward:  -5461.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|3         | 39/1000 [12:14:20<243:07:45, 910.79s/episodes] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  4538 epsilon:  0.7401890541913636 episode_reward:  -2073.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|4         | 40/1000 [12:26:18<227:24:57, 852.81s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  7856 epsilon:  0.7364881089204068 episode_reward:  -3732.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|4         | 41/1000 [12:47:03<258:32:46, 970.56s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  13609 epsilon:  0.7328056683758049 episode_reward:  -6609.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|4         | 42/1000 [12:56:01<223:45:23, 840.84s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5897 epsilon:  0.7291416400339258 episode_reward:  -2753.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|4         | 43/1000 [13:40:01<367:00:25, 1380.59s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  28826 epsilon:  0.7254959318337562 episode_reward:  -14217.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|4         | 44/1000 [13:50:40<307:33:55, 1158.20s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6916 epsilon:  0.7218684521745874 episode_reward:  -3262.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|4         | 45/1000 [14:13:49<325:35:58, 1227.39s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  14992 epsilon:  0.7182591099137144 episode_reward:  -7300.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|4         | 46/1000 [14:55:27<426:16:34, 1608.59s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  27035 epsilon:  0.7146678143641458 episode_reward:  -13322.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|4         | 47/1000 [15:03:55<338:25:07, 1278.39s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5025 epsilon:  0.7110944752923251 episode_reward:  -2317.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|4         | 48/1000 [15:14:49<288:31:37, 1091.07s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6446 epsilon:  0.7075390029158635 episode_reward:  -3027.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|4         | 49/1000 [15:23:36<243:30:53, 921.82s/episodes] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5191 epsilon:  0.7040013079012841 episode_reward:  -2400.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|5         | 50/1000 [15:33:23<216:43:24, 821.27s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5330 epsilon:  0.7004813013617777 episode_reward:  -2469.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|5         | 51/1000 [15:49:27<227:48:31, 864.19s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  9439 epsilon:  0.6969788948549688 episode_reward:  -4524.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|5         | 52/1000 [16:28:27<344:08:58, 1306.90s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  22866 epsilon:  0.693494000380694 episode_reward:  -11237.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|5         | 53/1000 [17:39:51<578:41:54, 2199.91s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  41845 epsilon:  0.6900265303787905 episode_reward:  -20727.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|5         | 54/1000 [17:58:58<495:05:23, 1884.06s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  11188 epsilon:  0.6865763977268965 episode_reward:  -5398.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|5         | 55/1000 [19:19:45<727:53:43, 2772.93s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  47225 epsilon:  0.683143515738262 episode_reward:  -23417.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|5         | 56/1000 [19:28:36<550:45:43, 2100.36s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  5159 epsilon:  0.6797277981595707 episode_reward:  -2384.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|5         | 57/1000 [20:35:21<699:53:27, 2671.91s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  39054 epsilon:  0.6763291591687729 episode_reward:  -19331.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|5         | 58/1000 [20:46:10<540:22:05, 2065.10s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6348 epsilon:  0.672947513372929 episode_reward:  -2978.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|5         | 59/1000 [20:57:17<430:09:18, 1645.65s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  6477 epsilon:  0.6695827758060644 episode_reward:  -3043.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|6         | 60/1000 [21:31:23<461:04:13, 1765.80s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  19848 epsilon:  0.6662348619270341 episode_reward:  -9728.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|6         | 61/1000 [21:46:09<391:43:47, 1501.84s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  8607 epsilon:  0.6629036876173989 episode_reward:  -4108.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|6         | 62/1000 [22:06:48<370:43:42, 1422.84s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  11977 epsilon:  0.659589169179312 episode_reward:  -5793.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|6         | 63/1000 [22:56:25<491:42:06, 1889.14s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  28791 epsilon:  0.6562912233334154 episode_reward:  -14200.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|6         | 64/1000 [23:10:09<408:05:46, 1569.60s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  7962 epsilon:  0.6530097672167483 episode_reward:  -3785.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|6         | 65/1000 [23:24:30<352:24:28, 1356.86s/episodes]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps:  8321 epsilon:  0.6497447183806646 episode_reward:  -3965.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# For stats\n",
        "ep_rewards = [-200]\n",
        "\n",
        "# For more repetitive results\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# Create models folder\n",
        "date = time.strftime(\"%Y_%m_%d-%H_%M\")\n",
        "result_path = date + MODEL_NAME\n",
        "image_path = os.path.join('images', result_path)\n",
        "model_path = os.path.join('models', result_path)\n",
        "episode_path = os.path.join(image_path, 'episode_')\n",
        "if not os.path.isdir(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "if not os.path.isdir(image_path):\n",
        "    os.makedirs(image_path)\n",
        "\n",
        "os.makedirs(episode_path + str(1))\n",
        "for episode in range(int(EPISODES/50)):\n",
        "    os.makedirs(episode_path + str((episode+1)*50))\n",
        "\n",
        "# MAIN PROGRAM\n",
        "\n",
        "# A. Initialize DQNAgent (include policy network, target network, and replay memory capacity)\n",
        "agent = DQNAgent(STATE_SIZE, ACTION_SIZE)\n",
        "env = DroneEnv()\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'): #tqdms is a progress bar\n",
        "    \n",
        "    # // Update tensorboard step every episode\n",
        "    agent.tensorboard.step = episode\n",
        "\n",
        "    # // Restarting episode - reset episode reward and step number\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "    trajectory = []\n",
        "\n",
        "    # 1. Initialize the starting state\n",
        "    current_state = env.reset()\n",
        "    \n",
        "    # 2. Reset flag and start iterating until episode ends\n",
        "    done = False\n",
        "    while not done:\n",
        "        # a. Explore Exploit Tradeoff\n",
        "        # b. Execute the action in the environment\n",
        "        if episode == 1:\n",
        "          action = np.random.randint(0, ACTION_SIZE)\n",
        "        elif np.random.random() >= epsilon:\n",
        "            # Get action from Q table\n",
        "            action = np.argmax(agent.get_qs(current_state))\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, ACTION_SIZE)\n",
        "            # action = np.argmax(agent.get_qs(current_state))\n",
        "        \n",
        "        # c. Observe reward and next state\n",
        "        new_state, reward, done = env.step(action, current_state)\n",
        "        episode_reward += reward\n",
        "        \n",
        "        # // Render\n",
        "        # if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
        "        #     env.render()\n",
        "            \n",
        "        # d. Store experience in replay memory\n",
        "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
        "        \n",
        "        # e. Train the agent\n",
        "        agent.train(done, step)\n",
        "        current_state = new_state\n",
        "        if episode % 50 == 0 or episode == 1:\n",
        "            env.saveImage(image_path, episode, step)\n",
        "        step += 1\n",
        "        trajectory.append([env.agent_1.x,env.agent_1.y])\n",
        "\n",
        "\n",
        "    # 3. Tensorboard, append episode reward to a list and log stats (every given number of episodes)\n",
        "    ep_rewards.append(episode_reward)\n",
        "    agent.tensorboard.update_stats(epsilon = epsilon, steps = step, episode_reward = episode_reward, pathDist=current_state)\n",
        "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
        "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward)\n",
        "\n",
        "        # Save model, but only when min reward is greater or equal a set value\n",
        "        if episode % 50 == 0 or episode == 1:\n",
        "            agent.model.save(f'{model_path}/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
        "            agent.target_model.save(f'{model_path}/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}_target.model')\n",
        "\n",
        "    np.savetxt(f'{model_path}/{MODEL_NAME}_Episode{episode}.csv', trajectory, delimiter=\",\")\n",
        "    if epsilon > MIN_EPSILON:\n",
        "        epsilon *= EPSILON_DECAY\n",
        "        epsilon = max(MIN_EPSILON, epsilon)\n",
        "    print('steps: ', step, 'epsilon: ', epsilon, 'episode_reward: ', episode_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVwbTrfjXcBi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "93f50a786178c234f0b1bff3476f651981df4cf2b8433153d2ab7805044c9624"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
